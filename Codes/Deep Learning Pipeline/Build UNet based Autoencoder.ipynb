{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"PAVE2ABP_GitHub.ipynb","provenance":[],"collapsed_sections":["W9vTr2NhcGAA","EQbDnsIZUSx0","tgW7r0C9TuZk","914TjXSMEZzv"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sFyb0y7PUJOo"},"source":["# Build UNet based Autoencoder with Further Options"]},{"cell_type":"markdown","metadata":{"id":"W9vTr2NhcGAA"},"source":["# Test GPU (Optional)\n","Before Starting, kindly check the available GPU from the Google Server, GPU model and other related information. It might help!"]},{"cell_type":"code","metadata":{"id":"VUNJNtNxcFTF"},"source":["import torch\n","print(\"Is CUDA enabled GPU Available?\", torch.cuda.is_available())\n","print(\"GPU Number:\", torch.cuda.device_count())\n","print(\"Current GPU Index:\", torch.cuda.current_device())\n","print(\"GPU Type:\", torch.cuda.get_device_name(device=None))\n","print(\"GPU Capability:\", torch.cuda.get_device_capability(device=None))\n","print(\"Is GPU Initialized yet?\", torch.cuda.is_initialized())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EQbDnsIZUSx0"},"source":["# Connect to Google Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wg4kMbwxsH9h","executionInfo":{"status":"ok","timestamp":1624646150047,"user_tz":-180,"elapsed":174685,"user":{"displayName":"Sakib Mahmud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8lG2uTygQr7y6fmQUo67XXUtrCVGaEakj_P33Ft8=s64","userId":"03961007737707022852"}},"outputId":"b53c9392-fc63-4c26-d838-2d8279c658e6"},"source":["from google.colab import drive\n","drive.mount('/content/GDrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/GDrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GpdYZ2x3MbTd"},"source":["Move to the Target Directory"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Efh543dRsQga","executionInfo":{"status":"ok","timestamp":1624646150705,"user_tz":-180,"elapsed":668,"user":{"displayName":"Sakib Mahmud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8lG2uTygQr7y6fmQUo67XXUtrCVGaEakj_P33Ft8=s64","userId":"03961007737707022852"}},"outputId":"2486ca90-199e-4113-d3c8-c438bcf5069d"},"source":["%cd /content/GDrive/MyDrive"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/GDrive/MyDrive/Colab_Notebooks/Research/PPG2ABP\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_ZtMdxE7MeXX"},"source":["List the Files and Folders Located in the Current Directory"]},{"cell_type":"code","metadata":{"id":"eM92ZPcisMK1"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ano69se-e3QY"},"source":["## Evaluation of Predicting BP from PPG and ECG\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tgW7r0C9TuZk"},"source":["#Import Libraries"]},{"cell_type":"code","metadata":{"id":"eMhBhz1CrMb3"},"source":["import os\n","import h5py\n","import scipy\n","import random\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import seaborn as sn\n","from tqdm import tqdm\n","import scipy.io as sio\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n","from keras.layers import Input, Conv1D, MaxPooling1D, UpSampling1D, concatenate, BatchNormalization, Activation, add\n","from keras.layers import Conv2D, MaxPooling2D, Reshape, Flatten, Dense\n","from keras.models import Model, model_from_json\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from predict_test import predict_test_data\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from UNet_1DCNN import UNet\n","from pathlib import Path\n","from Prepare_Train_Dictionary import prepareTrainDict\n","%matplotlib inline\n","sns.set_style('white')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ue1DYnPce5vM"},"source":["# Import Data Here and Process"]},{"cell_type":"markdown","metadata":{"id":"914TjXSMEZzv"},"source":["# Build UNet based Autoencoder"]},{"cell_type":"markdown","metadata":{"id":"n1Z58KJofoms"},"source":["Configurations"]},{"cell_type":"code","metadata":{"id":"eUieithdSOCR"},"source":["num_channel = 4       # 1 to 4\n","feature_number = 1024 # Number of Features to be Extracted\n","signal_length = 1024  # Signal Length\n","model_depth = 5       # Number of layers in the Model\n","model_width = 128     # Number of Filters or Kernels in the input layer of the model\n","kernel_size = 3       # Size of the Kernel throughout the network\n","D_S = 0               # Use Deep Supervision or not (0: No, 1: Yes)\n","A_E = 1               # Use the Segmentation Model UNet as an Autoencoder to Extract Features (0: No, 1: Yes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cb3GZ_GRgNKs"},"source":["Prepare Data for Deep Supervision (if used)"]},{"cell_type":"code","metadata":{"id":"kAMRPULElHV0"},"source":["def prepareTrainDict(y, model_depth, signal_length):\n","  def approximate(inp, w_len, signal_length):\n","    op = np.zeros((len(inp),signal_length//w_len))\n","    for i in range(0,signal_length,w_len):\n","      try:\n","        op[:,i//w_len] = np.mean(inp[:,i:i+w_len],axis=1)\n","      except Exception as e:\n","        print(e)\n","        print(i)\n","  \t\n","    return op\n","\n","  out = {}\n","  Y_Train_dict = {}\n","  out['out'] = np.array(y)\n","  Y_Train_dict['out'] = out['out']\n","  for i in range(1, (model_depth+1)):\n","    name = f'level{i}'\n","    out[name] = np.expand_dims(approximate(np.squeeze(y), 2**i, signal_length),axis = 2)\n","    Y_Train_dict[f'level{i}'] = out[f'level{i}']\n","  \n","  return out, Y_Train_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"66OX7MCDgQkj"},"source":["Create the Model based on the Configurations set prior"]},{"cell_type":"code","metadata":{"id":"J7FBpEgUEZzy"},"source":["# Build model for PPG2BP - Shallow-Wide U-net as AutoEncoder\n","Model = UNet(signal_length, model_depth, num_channel, model_width, kernel_size, D_S, A_E, feature_number)\n","Model.compile(optimizer='adam', loss='mse', metrics=['mae'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PP7L1a1RgVqp"},"source":["Train Model"]},{"cell_type":"code","metadata":{"id":"lfxxoDlwlHV1"},"source":["if D_S == 1: # AutoEncoder with Deep Supervision\n","  # Prepare Dictionary for Parameters for Deep Supervision\n","  X_Train1 = X_Train\n","  X_Test1 = X_Test\n","  X_Val1 = X_Val\n","  [Y_Train1, Y_Train_dict] = prepareTrainDict(Y_Train, model_depth, signal_length)\n","  [Y_Test1, Y_Test_dict] = prepareTrainDict(Y_Test, model_depth, signal_length)\n","  [Y_Val1, Y_Val_dict] = prepareTrainDict(Y_Val, model_depth, signal_length)\n","  # Create Losses for D_S Levels\n","  loss_weights = np.zeros(PPG2BP_model_depth)\n","\n","  for i in range(0, PPG2BP_model_depth):\n","    loss_weights[i] = 1-(i*0.1)\n","   \n","  loss_weights\n","  # Train Network\n","  callbacks = [EarlyStopping(monitor='val_out_loss', patience=20, mode='min'), ModelCheckpoint('Result.h5', verbose=1, monitor='val_out_loss', save_best_only=True, mode='min')]\n","  Model.fit(X_Train1, Y_Train_dict, epochs=200, batch_size= 128, verbose=1, validation_data= (X_Val1, Y_Val_dict), shuffle= True, callbacks= callbacks)\n","\n","elif D_S == 0: # AutoEncoder without Deep Supervision\n","  callbacks = [EarlyStopping(monitor='val_loss', patience=20, mode='min'), ModelCheckpoint('Result.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='min')]\n","  Model.fit(X_Train, Y_Train, epochs= 200, batch_size= 128, verbose=1, validation_data= (X_Val, Y_Val), shuffle= True, callbacks= callbacks)\n","\n","# Validation Data is set to independent here by default. Use \"Validation_split\" otherwise."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2SyH_qfRgtmT"},"source":["Extrac Feature from the AutoEncoder"]},{"cell_type":"code","metadata":{"id":"2UOmWXhq5Uuz"},"source":["Feature_Extractor = Model(inputs = Model.input, outputs = Model.get_layer('features').output)\n","#\n","Train_Features = Feature_Extractor.predict(X_Train1)\n","Test_Features = Feature_Extractor.predict(X_Test1)"],"execution_count":null,"outputs":[]}]}
